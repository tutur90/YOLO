{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_0hLTlJz3fy",
        "outputId": "b57c2460-c833-4499-8133-9a1285d3b330"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO(\"yolov8x-obb.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "nWY7diVQz8sU"
      },
      "outputs": [],
      "source": [
        "def predict(chosen_model, img, classes=[], conf=0.5):\n",
        "    if classes:\n",
        "        results = chosen_model.predict(img, classes=classes, conf=conf, persist=True)\n",
        "    else:\n",
        "        results = chosen_model.predict(img, conf=conf, persist=True)\n",
        "\n",
        "    return results\n",
        "\n",
        "def predict_and_detect(chosen_model, img, classes=[], conf=0.5, rectangle_thickness=2, text_thickness=1):\n",
        "    results = predict(chosen_model, img, classes, conf=conf)\n",
        "    for result in results:\n",
        "        for box in result.boxes:\n",
        "            cv2.rectangle(img, (int(box.xyxy[0][0]), int(box.xyxy[0][1])),\n",
        "                          (int(box.xyxy[0][2]), int(box.xyxy[0][3])), (255, 0, 0), rectangle_thickness)\n",
        "            cv2.putText(img, f\"{result.names[int(box.cls[0])]}\",\n",
        "                        (int(box.xyxy[0][0]), int(box.xyxy[0][1]) - 10),\n",
        "                        cv2.FONT_HERSHEY_PLAIN, 1, (255, 0, 0), text_thickness)\n",
        "    return img, results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "bKKrOzcu0ydD"
      },
      "outputs": [],
      "source": [
        "# defining function for creating a writer (for mp4 videos)\n",
        "def create_video_writer(video_cap, output_filename):\n",
        "    # grab the width, height, and fps of the frames in the video stream.\n",
        "    frame_width = int(video_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    frame_height = int(video_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = int(video_cap.get(cv2.CAP_PROP_FPS))\n",
        "    # initialize the FourCC and a video writer object\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n",
        "    writer = cv2.VideoWriter(output_filename, fourcc, fps,\n",
        "                             (frame_width, frame_height))\n",
        "    return writer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "D8HgSQSO04HQ"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "'\u001b[31m\u001b[1mpersist\u001b[0m' is not a valid YOLO argument. \n\n    Arguments received: ['yolo', '--f=/home/arthur/.local/share/jupyter/runtime/kernel-v2-1015791bzW9r5Lfbkh.json']. Ultralytics 'yolo' commands use the following syntax:\n\n        yolo TASK MODE ARGS\n\n        Where   TASK (optional) is one of {'detect', 'classify', 'obb', 'pose', 'segment'}\n                MODE (required) is one of {'train', 'export', 'predict', 'track', 'val', 'benchmark'}\n                ARGS (optional) are any number of custom 'arg=value' pairs like 'imgsz=320' that override defaults.\n                    See all ARGS at https://docs.ultralytics.com/usage/cfg or with 'yolo cfg'\n\n    1. Train a detection model for 10 epochs with an initial learning_rate of 0.01\n        yolo train data=coco8.yaml model=yolov8n.pt epochs=10 lr0=0.01\n\n    2. Predict a YouTube video using a pretrained segmentation model at image size 320:\n        yolo predict model=yolov8n-seg.pt source='https://youtu.be/LNwODJXcvt4' imgsz=320\n\n    3. Val a pretrained detection model at batch-size 1 and image size 640:\n        yolo val model=yolov8n.pt data=coco8.yaml batch=1 imgsz=640\n\n    4. Export a YOLOv8n classification model to ONNX format at image size 224 by 128 (no TASK required)\n        yolo export model=yolov8n-cls.pt format=onnx imgsz=224,128\n\n    6. Explore your datasets using semantic search and SQL with a simple GUI powered by Ultralytics Explorer API\n        yolo explorer\n\n    5. Run special commands:\n        yolo help\n        yolo checks\n        yolo version\n        yolo settings\n        yolo copy-cfg\n        yolo cfg\n\n    Docs: https://docs.ultralytics.com\n    Community: https://community.ultralytics.com\n    GitHub: https://github.com/ultralytics/ultralytics\n     (<string>)",
          "output_type": "error",
          "traceback": [
            "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
            "\u001b[0m  File \u001b[1;32m~/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3553\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
            "\u001b[0m  Cell \u001b[1;32mIn[7], line 13\u001b[0m\n    result_img, _ = predict_and_detect(model, img, classes=[], conf=0.1)\u001b[0m\n",
            "\u001b[0m  Cell \u001b[1;32mIn[5], line 10\u001b[0m in \u001b[1;35mpredict_and_detect\u001b[0m\n    results = predict(chosen_model, img, classes, conf=conf)\u001b[0m\n",
            "\u001b[0m  Cell \u001b[1;32mIn[5], line 5\u001b[0m in \u001b[1;35mpredict\u001b[0m\n    results = chosen_model.predict(img, conf=conf, persist=True)\u001b[0m\n",
            "\u001b[0m  File \u001b[1;32m~/.local/lib/python3.10/site-packages/ultralytics/engine/model.py:447\u001b[0m in \u001b[1;35mpredict\u001b[0m\n    self.predictor.args = get_cfg(self.predictor.args, args)\u001b[0m\n",
            "\u001b[0m  File \u001b[1;32m~/.local/lib/python3.10/site-packages/ultralytics/cfg/__init__.py:212\u001b[0m in \u001b[1;35mget_cfg\u001b[0m\n    check_dict_alignment(cfg, overrides)\u001b[0m\n",
            "\u001b[0;36m  File \u001b[0;32m~/.local/lib/python3.10/site-packages/ultralytics/cfg/__init__.py:323\u001b[0;36m in \u001b[0;35mcheck_dict_alignment\u001b[0;36m\n\u001b[0;31m    raise SyntaxError(string + CLI_HELP_MSG) from e\u001b[0;36m\n",
            "\u001b[0;36m  File \u001b[0;32m<string>\u001b[0;36m\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m '\u001b[31m\u001b[1mpersist\u001b[0m' is not a valid YOLO argument. \n\n    Arguments received: ['yolo', '--f=/home/arthur/.local/share/jupyter/runtime/kernel-v2-1015791bzW9r5Lfbkh.json']. Ultralytics 'yolo' commands use the following syntax:\n\n        yolo TASK MODE ARGS\n\n        Where   TASK (optional) is one of {'detect', 'classify', 'obb', 'pose', 'segment'}\n                MODE (required) is one of {'train', 'export', 'predict', 'track', 'val', 'benchmark'}\n                ARGS (optional) are any number of custom 'arg=value' pairs like 'imgsz=320' that override defaults.\n                    See all ARGS at https://docs.ultralytics.com/usage/cfg or with 'yolo cfg'\n\n    1. Train a detection model for 10 epochs with an initial learning_rate of 0.01\n        yolo train data=coco8.yaml model=yolov8n.pt epochs=10 lr0=0.01\n\n    2. Predict a YouTube video using a pretrained segmentation model at image size 320:\n        yolo predict model=yolov8n-seg.pt source='https://youtu.be/LNwODJXcvt4' imgsz=320\n\n    3. Val a pretrained detection model at batch-size 1 and image size 640:\n        yolo val model=yolov8n.pt data=coco8.yaml batch=1 imgsz=640\n\n    4. Export a YOLOv8n classification model to ONNX format at image size 224 by 128 (no TASK required)\n        yolo export model=yolov8n-cls.pt format=onnx imgsz=224,128\n\n    6. Explore your datasets using semantic search and SQL with a simple GUI powered by Ultralytics Explorer API\n        yolo explorer\n\n    5. Run special commands:\n        yolo help\n        yolo checks\n        yolo version\n        yolo settings\n        yolo copy-cfg\n        yolo cfg\n\n    Docs: https://docs.ultralytics.com\n    Community: https://community.ultralytics.com\n    GitHub: https://github.com/ultralytics/ultralytics\n    \n"
          ]
        }
      ],
      "source": [
        "output_filename = \"Annoted_video.mp4\"\n",
        "\n",
        "cap = cv2.VideoCapture(output_filename)\n",
        "\n",
        "writer = create_video_writer(cap, output_filename)\n",
        "\n",
        "video_path = r\"./DJI_0015.MOV\"\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "while True:\n",
        "    success, img = cap.read()\n",
        "    if not success:\n",
        "        break\n",
        "    result_img, _ = predict_and_detect(model, img, classes=[], conf=0.1)\n",
        "    writer.write(result_img)\n",
        "    cv2.imshow(\"Image\", cv2.resize(result_img, (1920, 1080)) )\n",
        "\n",
        "    cv2.waitKey(1)\n",
        "writer.release()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "0: 352x640 (no detections), 405.8ms\n",
            "Speed: 2.0ms preprocess, 405.8ms inference, 0.7ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 (no detections), 350.2ms\n",
            "Speed: 2.0ms preprocess, 350.2ms inference, 1.1ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 (no detections), 397.8ms\n",
            "Speed: 1.9ms preprocess, 397.8ms inference, 1.1ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 (no detections), 312.8ms\n",
            "Speed: 2.1ms preprocess, 312.8ms inference, 1.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 (no detections), 292.8ms\n",
            "Speed: 2.5ms preprocess, 292.8ms inference, 1.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 (no detections), 317.4ms\n",
            "Speed: 2.0ms preprocess, 317.4ms inference, 1.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 (no detections), 319.3ms\n",
            "Speed: 2.1ms preprocess, 319.3ms inference, 1.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 (no detections), 321.8ms\n",
            "Speed: 2.3ms preprocess, 321.8ms inference, 1.1ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 (no detections), 331.9ms\n",
            "Speed: 3.2ms preprocess, 331.9ms inference, 1.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 (no detections), 360.6ms\n",
            "Speed: 2.6ms preprocess, 360.6ms inference, 1.1ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 (no detections), 318.1ms\n",
            "Speed: 2.3ms preprocess, 318.1ms inference, 0.6ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 (no detections), 390.3ms\n",
            "Speed: 2.0ms preprocess, 390.3ms inference, 1.1ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 (no detections), 376.4ms\n",
            "Speed: 2.2ms preprocess, 376.4ms inference, 1.1ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 (no detections), 350.3ms\n",
            "Speed: 2.1ms preprocess, 350.3ms inference, 1.1ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 (no detections), 305.8ms\n",
            "Speed: 2.4ms preprocess, 305.8ms inference, 1.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 (no detections), 321.6ms\n",
            "Speed: 2.4ms preprocess, 321.6ms inference, 1.1ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 (no detections), 346.2ms\n",
            "Speed: 2.2ms preprocess, 346.2ms inference, 1.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 (no detections), 397.3ms\n",
            "Speed: 3.0ms preprocess, 397.3ms inference, 0.8ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 (no detections), 314.5ms\n",
            "Speed: 3.1ms preprocess, 314.5ms inference, 0.6ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 (no detections), 293.4ms\n",
            "Speed: 13.0ms preprocess, 293.4ms inference, 0.7ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 (no detections), 317.3ms\n",
            "Speed: 3.1ms preprocess, 317.3ms inference, 1.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 (no detections), 306.1ms\n",
            "Speed: 2.6ms preprocess, 306.1ms inference, 0.6ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 (no detections), 303.4ms\n",
            "Speed: 5.7ms preprocess, 303.4ms inference, 1.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 (no detections), 306.0ms\n",
            "Speed: 2.3ms preprocess, 306.0ms inference, 1.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 (no detections), 293.4ms\n",
            "Speed: 2.5ms preprocess, 293.4ms inference, 1.1ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 (no detections), 333.8ms\n",
            "Speed: 2.5ms preprocess, 333.8ms inference, 1.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 (no detections), 399.8ms\n",
            "Speed: 2.0ms preprocess, 399.8ms inference, 1.1ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 (no detections), 276.2ms\n",
            "Speed: 2.3ms preprocess, 276.2ms inference, 1.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 (no detections), 339.6ms\n",
            "Speed: 2.6ms preprocess, 339.6ms inference, 5.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 (no detections), 373.4ms\n",
            "Speed: 2.0ms preprocess, 373.4ms inference, 1.1ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 (no detections), 321.5ms\n",
            "Speed: 2.8ms preprocess, 321.5ms inference, 0.6ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 (no detections), 342.8ms\n",
            "Speed: 2.6ms preprocess, 342.8ms inference, 1.1ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 (no detections), 332.3ms\n",
            "Speed: 2.3ms preprocess, 332.3ms inference, 1.1ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 (no detections), 284.2ms\n",
            "Speed: 2.2ms preprocess, 284.2ms inference, 0.7ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 (no detections), 313.2ms\n",
            "Speed: 10.8ms preprocess, 313.2ms inference, 0.6ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 (no detections), 304.5ms\n",
            "Speed: 1.8ms preprocess, 304.5ms inference, 0.6ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 (no detections), 311.5ms\n",
            "Speed: 2.2ms preprocess, 311.5ms inference, 0.7ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 (no detections), 335.3ms\n",
            "Speed: 2.3ms preprocess, 335.3ms inference, 1.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 (no detections), 411.7ms\n",
            "Speed: 1.9ms preprocess, 411.7ms inference, 1.7ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 (no detections), 355.1ms\n",
            "Speed: 3.0ms preprocess, 355.1ms inference, 1.1ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 (no detections), 328.5ms\n",
            "Speed: 2.4ms preprocess, 328.5ms inference, 0.8ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 (no detections), 362.8ms\n",
            "Speed: 2.7ms preprocess, 362.8ms inference, 1.1ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 (no detections), 332.1ms\n",
            "Speed: 2.8ms preprocess, 332.1ms inference, 1.1ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 (no detections), 280.4ms\n",
            "Speed: 2.1ms preprocess, 280.4ms inference, 1.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 (no detections), 278.8ms\n",
            "Speed: 2.4ms preprocess, 278.8ms inference, 0.6ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 (no detections), 317.6ms\n",
            "Speed: 2.2ms preprocess, 317.6ms inference, 1.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 (no detections), 307.2ms\n",
            "Speed: 1.9ms preprocess, 307.2ms inference, 1.1ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 (no detections), 402.2ms\n",
            "Speed: 2.5ms preprocess, 402.2ms inference, 1.1ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 (no detections), 317.9ms\n",
            "Speed: 2.1ms preprocess, 317.9ms inference, 0.6ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 (no detections), 271.9ms\n",
            "Speed: 2.1ms preprocess, 271.9ms inference, 0.6ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 (no detections), 291.7ms\n",
            "Speed: 2.0ms preprocess, 291.7ms inference, 0.6ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 (no detections), 311.2ms\n",
            "Speed: 2.5ms preprocess, 311.2ms inference, 1.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 (no detections), 313.3ms\n",
            "Speed: 1.8ms preprocess, 313.3ms inference, 3.9ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 (no detections), 341.2ms\n",
            "Speed: 1.9ms preprocess, 341.2ms inference, 0.6ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 (no detections), 345.1ms\n",
            "Speed: 2.5ms preprocess, 345.1ms inference, 0.6ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 (no detections), 358.9ms\n",
            "Speed: 2.9ms preprocess, 358.9ms inference, 0.7ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 (no detections), 320.9ms\n",
            "Speed: 2.1ms preprocess, 320.9ms inference, 1.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 (no detections), 288.4ms\n",
            "Speed: 2.0ms preprocess, 288.4ms inference, 0.6ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 (no detections), 283.2ms\n",
            "Speed: 1.9ms preprocess, 283.2ms inference, 0.6ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 (no detections), 301.7ms\n",
            "Speed: 23.8ms preprocess, 301.7ms inference, 1.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 (no detections), 320.7ms\n",
            "Speed: 1.9ms preprocess, 320.7ms inference, 1.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 (no detections), 300.6ms\n",
            "Speed: 2.4ms preprocess, 300.6ms inference, 1.1ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 (no detections), 413.2ms\n",
            "Speed: 1.9ms preprocess, 413.2ms inference, 1.1ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 (no detections), 269.8ms\n",
            "Speed: 2.2ms preprocess, 269.8ms inference, 0.6ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 (no detections), 293.1ms\n",
            "Speed: 2.6ms preprocess, 293.1ms inference, 1.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 (no detections), 353.4ms\n",
            "Speed: 1.7ms preprocess, 353.4ms inference, 0.6ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 (no detections), 345.5ms\n",
            "Speed: 2.4ms preprocess, 345.5ms inference, 1.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 (no detections), 314.1ms\n",
            "Speed: 2.0ms preprocess, 314.1ms inference, 1.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 (no detections), 411.4ms\n",
            "Speed: 1.9ms preprocess, 411.4ms inference, 0.8ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 (no detections), 350.6ms\n",
            "Speed: 2.1ms preprocess, 350.6ms inference, 0.6ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 (no detections), 288.9ms\n",
            "Speed: 12.2ms preprocess, 288.9ms inference, 1.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 (no detections), 309.5ms\n",
            "Speed: 2.0ms preprocess, 309.5ms inference, 1.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 (no detections), 327.3ms\n",
            "Speed: 2.9ms preprocess, 327.3ms inference, 1.7ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 (no detections), 319.2ms\n",
            "Speed: 2.1ms preprocess, 319.2ms inference, 0.6ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 (no detections), 375.7ms\n",
            "Speed: 2.0ms preprocess, 375.7ms inference, 1.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 (no detections), 317.5ms\n",
            "Speed: 2.5ms preprocess, 317.5ms inference, 1.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 (no detections), 359.9ms\n",
            "Speed: 2.1ms preprocess, 359.9ms inference, 1.6ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 (no detections), 317.9ms\n",
            "Speed: 2.6ms preprocess, 317.9ms inference, 1.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 (no detections), 315.8ms\n",
            "Speed: 2.3ms preprocess, 315.8ms inference, 1.1ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 (no detections), 312.3ms\n",
            "Speed: 10.5ms preprocess, 312.3ms inference, 1.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 (no detections), 348.8ms\n",
            "Speed: 2.7ms preprocess, 348.8ms inference, 1.1ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 (no detections), 327.5ms\n",
            "Speed: 2.0ms preprocess, 327.5ms inference, 1.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 (no detections), 404.2ms\n",
            "Speed: 2.5ms preprocess, 404.2ms inference, 1.3ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 (no detections), 412.4ms\n",
            "Speed: 2.1ms preprocess, 412.4ms inference, 1.2ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 (no detections), 367.2ms\n",
            "Speed: 2.4ms preprocess, 367.2ms inference, 1.1ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 (no detections), 379.8ms\n",
            "Speed: 2.2ms preprocess, 379.8ms inference, 1.1ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 (no detections), 281.2ms\n",
            "Speed: 2.7ms preprocess, 281.2ms inference, 1.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 (no detections), 328.9ms\n",
            "Speed: 3.4ms preprocess, 328.9ms inference, 0.6ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 (no detections), 375.4ms\n",
            "Speed: 1.9ms preprocess, 375.4ms inference, 1.3ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 (no detections), 317.6ms\n",
            "Speed: 2.1ms preprocess, 317.6ms inference, 0.6ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 (no detections), 307.8ms\n",
            "Speed: 2.7ms preprocess, 307.8ms inference, 0.6ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 (no detections), 428.2ms\n",
            "Speed: 2.0ms preprocess, 428.2ms inference, 1.2ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 (no detections), 358.2ms\n",
            "Speed: 2.4ms preprocess, 358.2ms inference, 0.6ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[5], line 46\u001b[0m\n\u001b[1;32m     43\u001b[0m frame_number \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Run YOLOv8 tracking on the frame, persisting tracks between frames\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# detect only classes=[2,5,7], car, bus, truck\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpersist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclasses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Visualize the results on the frame\u001b[39;00m\n\u001b[1;32m     50\u001b[0m annotated_frame \u001b[38;5;241m=\u001b[39m results[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mplot()\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ultralytics/engine/model.py:492\u001b[0m, in \u001b[0;36mModel.track\u001b[0;34m(self, source, stream, persist, **kwargs)\u001b[0m\n\u001b[1;32m    490\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# batch-size 1 for tracking in videos\u001b[39;00m\n\u001b[1;32m    491\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrack\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 492\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ultralytics/engine/model.py:452\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_prompts\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mset_prompts(prompts)\n\u001b[0;32m--> 452\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mpredict_cli(source\u001b[38;5;241m=\u001b[39msource) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ultralytics/engine/predictor.py:168\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[0;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 168\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py:35\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m---> 35\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ultralytics/engine/predictor.py:248\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[0;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m--> 248\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39membed:\n\u001b[1;32m    250\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m [preds] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(preds, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m preds  \u001b[38;5;66;03m# yield embedding tensors\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ultralytics/engine/predictor.py:142\u001b[0m, in \u001b[0;36mBasePredictor.inference\u001b[0;34m(self, im, *args, **kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Runs inference on a given image using the specified model and arguments.\"\"\"\u001b[39;00m\n\u001b[1;32m    137\u001b[0m visualize \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    138\u001b[0m     increment_path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_dir \u001b[38;5;241m/\u001b[39m Path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mstem, mkdir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mvisualize \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_type\u001b[38;5;241m.\u001b[39mtensor)\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    141\u001b[0m )\n\u001b[0;32m--> 142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ultralytics/nn/autobackend.py:453\u001b[0m, in \u001b[0;36mAutoBackend.forward\u001b[0;34m(self, im, augment, visualize, embed)\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;66;03m# PyTorch\u001b[39;00m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpt \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn_module:\n\u001b[0;32m--> 453\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;66;03m# TorchScript\u001b[39;00m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjit:\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ultralytics/nn/tasks.py:89\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 89\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ultralytics/nn/tasks.py:107\u001b[0m, in \u001b[0;36mBaseModel.predict\u001b[0;34m(self, x, profile, visualize, augment, embed)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m augment:\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_augment(x)\n\u001b[0;32m--> 107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ultralytics/nn/tasks.py:128\u001b[0m, in \u001b[0;36mBaseModel._predict_once\u001b[0;34m(self, x, profile, visualize, embed)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m profile:\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profile_one_layer(m, x, dt)\n\u001b[0;32m--> 128\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[1;32m    129\u001b[0m y\u001b[38;5;241m.\u001b[39mappend(x \u001b[38;5;28;01mif\u001b[39;00m m\u001b[38;5;241m.\u001b[39mi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ultralytics/nn/modules/block.py:595\u001b[0m, in \u001b[0;36mRepNCSPELAN4.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    594\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Forward pass through RepNCSPELAN4 layer.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 595\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    596\u001b[0m     y\u001b[38;5;241m.\u001b[39mextend((m(y[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv2, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv3])\n\u001b[1;32m    597\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv4(torch\u001b[38;5;241m.\u001b[39mcat(y, \u001b[38;5;241m1\u001b[39m))\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1514\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1511\u001b[0m             tracing_state\u001b[38;5;241m.\u001b[39mpop_scope()\n\u001b[1;32m   1512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m-> 1514\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrapped_call_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1515\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1516\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from ultralytics import YOLO\n",
        "\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "\n",
        "# object classes\n",
        "classNames = [\"person\", \"bicycle\", \"car\", \"motorbike\", \"aeroplane\", \"bus\", \"train\", \"truck\", \"boat\",\n",
        "              \"traffic light\", \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\", \"bird\", \"cat\",\n",
        "              \"dog\", \"horse\", \"sheep\", \"cow\", \"elephant\", \"bear\", \"zebra\", \"giraffe\", \"backpack\", \"umbrella\",\n",
        "              \"handbag\", \"tie\", \"suitcase\", \"frisbee\", \"skis\", \"snowboard\", \"sports ball\", \"kite\", \"baseball bat\",\n",
        "              \"baseball glove\", \"skateboard\", \"surfboard\", \"tennis racket\", \"bottle\", \"wine glass\", \"cup\",\n",
        "              \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\", \"apple\", \"sandwich\", \"orange\", \"broccoli\",\n",
        "              \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\", \"chair\", \"sofa\", \"pottedplant\", \"bed\",\n",
        "              \"diningtable\", \"toilet\", \"tvmonitor\", \"laptop\", \"mouse\", \"remote\", \"keyboard\", \"cell phone\",\n",
        "              \"microwave\", \"oven\", \"toaster\", \"sink\", \"refrigerator\", \"book\", \"clock\", \"vase\", \"scissors\",\n",
        "              \"teddy bear\", \"hair drier\", \"toothbrush\"\n",
        "              ]\n",
        "\n",
        "# Load the YOLOv8 model\n",
        "model = YOLO('yolov9c.pt')\n",
        "\n",
        "# Open the video file\n",
        "video_path = \"DJI_0015.MOV\"\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Store the track history\n",
        "track_history = defaultdict(lambda: [])\n",
        "# Store the class history\n",
        "cls_history = defaultdict(lambda: [])\n",
        "\n",
        "frame_number = 0\n",
        "\n",
        "# Loop through the video frames\n",
        "while cap.isOpened():\n",
        "    # Read a frame from the video\n",
        "    success, frame = cap.read()\n",
        "\n",
        "    if success:\n",
        "        frame_number += 1\n",
        "        # Run YOLOv8 tracking on the frame, persisting tracks between frames\n",
        "        # detect only classes=[2,5,7], car, bus, truck\n",
        "        results = model.track(frame, persist=True, classes=[], conf=0.1)\n",
        "        \n",
        "\n",
        "        # Visualize the results on the frame\n",
        "        annotated_frame = results[0].plot()\n",
        "        # Get the boxes and track IDs\n",
        "        \n",
        "        if results[0].boxes.id is not None:\n",
        "        \n",
        "            boxes = results[0].boxes.xywh.cpu()\n",
        "            clss = results[0].boxes.cls\n",
        "            track_ids = results[0].boxes.id.int().cpu().tolist()\n",
        "\n",
        "\n",
        "            # Plot the tracks\n",
        "            for box, track_id, cls in zip(boxes, track_ids, clss):\n",
        "                x, y, w, h = box\n",
        "                track = track_history[track_id]\n",
        "                ch = cls_history[track_id]\n",
        "                track.append((float(x), float(y)))  # x, y center point\n",
        "                ch.append((classNames[int(cls)], frame_number))\n",
        "                # class name\n",
        "                # print(\"Class name -->\", classNames[int(cls)])\n",
        "\n",
        "                # Draw the tracking lines\n",
        "                points = np.hstack(track).astype(np.int32).reshape((-1, 1, 2))\n",
        "                cv2.polylines(annotated_frame, [points], isClosed=False, color=(255, 255, 0), thickness=2)\n",
        "\n",
        "        # Display the annotated frame\n",
        "        cv2.imshow(\"YOLOv8 Tracking\", cv2.resize(frame, (1920, 1080)))\n",
        "\n",
        "        # Break the loop if 'q' is pressed\n",
        "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
        "            break\n",
        "    else:\n",
        "        # Break the loop if the end of the video is reached\n",
        "        break\n",
        "\n",
        "# Release the video capture object and close the display window\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "0: 352x640 2 trucks, 853.3ms\n",
            "Speed: 15.2ms preprocess, 853.3ms inference, 3.8ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 2 trucks, 739.7ms\n",
            "Speed: 4.1ms preprocess, 739.7ms inference, 3.3ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 1 truck, 754.8ms\n",
            "Speed: 3.8ms preprocess, 754.8ms inference, 3.5ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 1 person, 2 trucks, 888.8ms\n",
            "Speed: 56.0ms preprocess, 888.8ms inference, 3.5ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 2 trucks, 793.5ms\n",
            "Speed: 6.6ms preprocess, 793.5ms inference, 3.2ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 1 person, 1 truck, 760.5ms\n",
            "Speed: 4.3ms preprocess, 760.5ms inference, 3.1ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 1 person, 1 truck, 760.0ms\n",
            "Speed: 3.5ms preprocess, 760.0ms inference, 3.2ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 1 person, 2 trucks, 782.4ms\n",
            "Speed: 50.1ms preprocess, 782.4ms inference, 2.9ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 1 person, 1 truck, 776.1ms\n",
            "Speed: 3.6ms preprocess, 776.1ms inference, 3.3ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 1 person, 1 truck, 1 tv, 703.8ms\n",
            "Speed: 5.6ms preprocess, 703.8ms inference, 2.8ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 1 person, 1 car, 1 truck, 1 tv, 711.4ms\n",
            "Speed: 5.1ms preprocess, 711.4ms inference, 2.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 2 trucks, 1 tv, 711.4ms\n",
            "Speed: 3.3ms preprocess, 711.4ms inference, 2.1ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 1 person, 2 trucks, 1 tv, 841.7ms\n",
            "Speed: 3.0ms preprocess, 841.7ms inference, 2.7ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 1 person, 2 trucks, 1 tv, 638.0ms\n",
            "Speed: 2.8ms preprocess, 638.0ms inference, 1.9ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 1 truck, 774.3ms\n",
            "Speed: 35.6ms preprocess, 774.3ms inference, 3.2ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 1 truck, 609.3ms\n",
            "Speed: 3.0ms preprocess, 609.3ms inference, 1.7ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 1 truck, 1 tv, 610.9ms\n",
            "Speed: 3.4ms preprocess, 610.9ms inference, 2.1ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 1 truck, 1 tv, 609.0ms\n",
            "Speed: 4.3ms preprocess, 609.0ms inference, 1.6ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 1 truck, 1 tv, 770.4ms\n",
            "Speed: 3.4ms preprocess, 770.4ms inference, 3.7ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 1 car, 1 truck, 1 tv, 804.0ms\n",
            "Speed: 4.1ms preprocess, 804.0ms inference, 3.3ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 1 car, 2 trucks, 1 tv, 857.5ms\n",
            "Speed: 4.7ms preprocess, 857.5ms inference, 1.8ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 2 trucks, 651.4ms\n",
            "Speed: 3.2ms preprocess, 651.4ms inference, 1.8ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 1 truck, 1 tv, 728.0ms\n",
            "Speed: 5.3ms preprocess, 728.0ms inference, 3.3ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 2 trucks, 693.3ms\n",
            "Speed: 3.3ms preprocess, 693.3ms inference, 3.3ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 2 trucks, 594.2ms\n",
            "Speed: 3.0ms preprocess, 594.2ms inference, 3.2ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 1 truck, 612.6ms\n",
            "Speed: 3.1ms preprocess, 612.6ms inference, 1.7ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 2 trucks, 672.4ms\n",
            "Speed: 3.1ms preprocess, 672.4ms inference, 3.2ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 2 trucks, 700.9ms\n",
            "Speed: 3.9ms preprocess, 700.9ms inference, 3.8ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 2 trucks, 739.0ms\n",
            "Speed: 3.3ms preprocess, 739.0ms inference, 3.3ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 2 trucks, 662.7ms\n",
            "Speed: 4.3ms preprocess, 662.7ms inference, 3.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 1 truck, 705.3ms\n",
            "Speed: 3.1ms preprocess, 705.3ms inference, 3.3ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 1 truck, 732.7ms\n",
            "Speed: 3.1ms preprocess, 732.7ms inference, 4.6ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 1 truck, 621.4ms\n",
            "Speed: 3.1ms preprocess, 621.4ms inference, 1.7ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 2 trucks, 629.5ms\n",
            "Speed: 3.1ms preprocess, 629.5ms inference, 2.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 2 trucks, 728.9ms\n",
            "Speed: 3.7ms preprocess, 728.9ms inference, 1.8ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 1 truck, 733.8ms\n",
            "Speed: 3.8ms preprocess, 733.8ms inference, 3.3ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 1 truck, 800.9ms\n",
            "Speed: 52.9ms preprocess, 800.9ms inference, 2.8ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 1 truck, 698.2ms\n",
            "Speed: 4.2ms preprocess, 698.2ms inference, 3.4ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 1 truck, 672.4ms\n",
            "Speed: 3.2ms preprocess, 672.4ms inference, 2.9ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 1 truck, 655.8ms\n",
            "Speed: 3.3ms preprocess, 655.8ms inference, 2.3ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 2 trucks, 727.8ms\n",
            "Speed: 3.0ms preprocess, 727.8ms inference, 3.2ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 1 truck, 624.4ms\n",
            "Speed: 4.1ms preprocess, 624.4ms inference, 2.7ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 1 truck, 642.9ms\n",
            "Speed: 3.9ms preprocess, 642.9ms inference, 1.5ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 1 truck, 687.6ms\n",
            "Speed: 3.2ms preprocess, 687.6ms inference, 2.1ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 1 truck, 721.3ms\n",
            "Speed: 2.9ms preprocess, 721.3ms inference, 3.7ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 1 truck, 646.9ms\n",
            "Speed: 2.9ms preprocess, 646.9ms inference, 2.1ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 2 trucks, 635.6ms\n",
            "Speed: 3.3ms preprocess, 635.6ms inference, 1.9ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 2 trucks, 663.4ms\n",
            "Speed: 3.3ms preprocess, 663.4ms inference, 1.9ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 1 truck, 583.0ms\n",
            "Speed: 4.5ms preprocess, 583.0ms inference, 1.7ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 1 truck, 707.1ms\n",
            "Speed: 4.0ms preprocess, 707.1ms inference, 3.2ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 1 truck, 659.6ms\n",
            "Speed: 10.1ms preprocess, 659.6ms inference, 2.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 1 truck, 636.3ms\n",
            "Speed: 3.6ms preprocess, 636.3ms inference, 1.9ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 1 truck, 698.3ms\n",
            "Speed: 3.1ms preprocess, 698.3ms inference, 3.3ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 1 truck, 662.5ms\n",
            "Speed: 2.8ms preprocess, 662.5ms inference, 2.9ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 1 truck, 664.2ms\n",
            "Speed: 2.9ms preprocess, 664.2ms inference, 3.2ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 1 truck, 660.1ms\n",
            "Speed: 3.0ms preprocess, 660.1ms inference, 2.1ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 1 truck, 645.8ms\n",
            "Speed: 4.6ms preprocess, 645.8ms inference, 1.8ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 1 truck, 913.1ms\n",
            "Speed: 3.4ms preprocess, 913.1ms inference, 3.2ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 1 truck, 710.1ms\n",
            "Speed: 3.1ms preprocess, 710.1ms inference, 3.2ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 1 truck, 674.4ms\n",
            "Speed: 3.6ms preprocess, 674.4ms inference, 3.2ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 1 truck, 657.0ms\n",
            "Speed: 4.5ms preprocess, 657.0ms inference, 1.7ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 1 truck, 664.7ms\n",
            "Speed: 4.2ms preprocess, 664.7ms inference, 2.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 1 truck, 720.7ms\n",
            "Speed: 3.6ms preprocess, 720.7ms inference, 3.2ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 1 truck, 711.6ms\n",
            "Speed: 3.1ms preprocess, 711.6ms inference, 3.1ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 1 truck, 643.6ms\n",
            "Speed: 3.1ms preprocess, 643.6ms inference, 3.2ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 1 truck, 752.0ms\n",
            "Speed: 3.3ms preprocess, 752.0ms inference, 3.5ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 2 trucks, 643.0ms\n",
            "Speed: 3.0ms preprocess, 643.0ms inference, 3.3ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 2 trucks, 729.6ms\n",
            "Speed: 4.3ms preprocess, 729.6ms inference, 3.6ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 2 trucks, 726.3ms\n",
            "Speed: 4.0ms preprocess, 726.3ms inference, 2.6ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 2 trucks, 705.5ms\n",
            "Speed: 2.9ms preprocess, 705.5ms inference, 3.4ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 2 trucks, 712.6ms\n",
            "Speed: 2.8ms preprocess, 712.6ms inference, 1.6ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 2 trucks, 694.1ms\n",
            "Speed: 3.2ms preprocess, 694.1ms inference, 3.2ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 2 trucks, 649.9ms\n",
            "Speed: 3.3ms preprocess, 649.9ms inference, 1.7ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 2 trucks, 678.6ms\n",
            "Speed: 3.2ms preprocess, 678.6ms inference, 2.7ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 2 trucks, 706.8ms\n",
            "Speed: 3.1ms preprocess, 706.8ms inference, 2.8ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 1 truck, 641.2ms\n",
            "Speed: 3.1ms preprocess, 641.2ms inference, 2.6ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 1 truck, 826.8ms\n",
            "Speed: 3.5ms preprocess, 826.8ms inference, 1.9ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 1 truck, 711.6ms\n",
            "Speed: 4.9ms preprocess, 711.6ms inference, 3.2ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 1 truck, 750.0ms\n",
            "Speed: 3.2ms preprocess, 750.0ms inference, 3.2ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 1 truck, 802.8ms\n",
            "Speed: 3.1ms preprocess, 802.8ms inference, 3.2ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 1 truck, 684.8ms\n",
            "Speed: 4.8ms preprocess, 684.8ms inference, 2.3ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 1 truck, 663.0ms\n",
            "Speed: 4.4ms preprocess, 663.0ms inference, 2.7ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 1 truck, 623.1ms\n",
            "Speed: 3.4ms preprocess, 623.1ms inference, 1.6ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 1 truck, 722.3ms\n",
            "Speed: 4.1ms preprocess, 722.3ms inference, 3.3ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 1 truck, 766.3ms\n",
            "Speed: 3.8ms preprocess, 766.3ms inference, 3.8ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 1 truck, 801.9ms\n",
            "Speed: 49.3ms preprocess, 801.9ms inference, 3.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 1 truck, 694.2ms\n",
            "Speed: 5.0ms preprocess, 694.2ms inference, 2.9ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 1 truck, 843.8ms\n",
            "Speed: 3.3ms preprocess, 843.8ms inference, 2.6ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[26], line 25\u001b[0m\n\u001b[1;32m     21\u001b[0m success, frame \u001b[38;5;241m=\u001b[39m cap\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# Run YOLOv8 tracking on the frame, persisting tracks between frames\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpersist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m# Get the boxes and track IDs\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m# boxes = results[0].boxes.xywh.cpu()\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;66;03m# track_ids = results[0].boxes.id.int().cpu().tolist()\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m# Visualize the results on the frame\u001b[39;00m\n\u001b[1;32m     32\u001b[0m     annotated_frame \u001b[38;5;241m=\u001b[39m results[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mplot()\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ultralytics/engine/model.py:492\u001b[0m, in \u001b[0;36mModel.track\u001b[0;34m(self, source, stream, persist, **kwargs)\u001b[0m\n\u001b[1;32m    490\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# batch-size 1 for tracking in videos\u001b[39;00m\n\u001b[1;32m    491\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrack\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 492\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ultralytics/engine/model.py:452\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_prompts\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mset_prompts(prompts)\n\u001b[0;32m--> 452\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mpredict_cli(source\u001b[38;5;241m=\u001b[39msource) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ultralytics/engine/predictor.py:168\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[0;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 168\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py:35\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m---> 35\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ultralytics/engine/predictor.py:248\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[0;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m--> 248\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39membed:\n\u001b[1;32m    250\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m [preds] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(preds, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m preds  \u001b[38;5;66;03m# yield embedding tensors\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ultralytics/engine/predictor.py:142\u001b[0m, in \u001b[0;36mBasePredictor.inference\u001b[0;34m(self, im, *args, **kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Runs inference on a given image using the specified model and arguments.\"\"\"\u001b[39;00m\n\u001b[1;32m    137\u001b[0m visualize \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    138\u001b[0m     increment_path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_dir \u001b[38;5;241m/\u001b[39m Path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mstem, mkdir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mvisualize \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_type\u001b[38;5;241m.\u001b[39mtensor)\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    141\u001b[0m )\n\u001b[0;32m--> 142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ultralytics/nn/autobackend.py:453\u001b[0m, in \u001b[0;36mAutoBackend.forward\u001b[0;34m(self, im, augment, visualize, embed)\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;66;03m# PyTorch\u001b[39;00m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpt \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn_module:\n\u001b[0;32m--> 453\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;66;03m# TorchScript\u001b[39;00m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjit:\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ultralytics/nn/tasks.py:89\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 89\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ultralytics/nn/tasks.py:107\u001b[0m, in \u001b[0;36mBaseModel.predict\u001b[0;34m(self, x, profile, visualize, augment, embed)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m augment:\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_augment(x)\n\u001b[0;32m--> 107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ultralytics/nn/tasks.py:128\u001b[0m, in \u001b[0;36mBaseModel._predict_once\u001b[0;34m(self, x, profile, visualize, embed)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m profile:\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profile_one_layer(m, x, dt)\n\u001b[0;32m--> 128\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[1;32m    129\u001b[0m y\u001b[38;5;241m.\u001b[39mappend(x \u001b[38;5;28;01mif\u001b[39;00m m\u001b[38;5;241m.\u001b[39mi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ultralytics/nn/modules/block.py:596\u001b[0m, in \u001b[0;36mRepNCSPELAN4.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Forward pass through RepNCSPELAN4 layer.\"\"\"\u001b[39;00m\n\u001b[1;32m    595\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv1(x)\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m--> 596\u001b[0m \u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcv2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcv3\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv4(torch\u001b[38;5;241m.\u001b[39mcat(y, \u001b[38;5;241m1\u001b[39m))\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ultralytics/nn/modules/block.py:596\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Forward pass through RepNCSPELAN4 layer.\"\"\"\u001b[39;00m\n\u001b[1;32m    595\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv1(x)\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m--> 596\u001b[0m y\u001b[38;5;241m.\u001b[39mextend((\u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv2, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv3])\n\u001b[1;32m    597\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv4(torch\u001b[38;5;241m.\u001b[39mcat(y, \u001b[38;5;241m1\u001b[39m))\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ultralytics/nn/modules/block.py:254\u001b[0m, in \u001b[0;36mC3.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    253\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Forward pass through the CSP bottleneck with 2 convolutions.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcv3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ultralytics/nn/modules/conv.py:54\u001b[0m, in \u001b[0;36mConv.forward_fuse\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_fuse\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     53\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Perform transposed convolution of 2D data.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "from ultralytics import YOLO\n",
        "\n",
        "writer = create_video_writer(cap, output_filename)\n",
        "\n",
        "# Load the YOLOv8 model\n",
        "model = YOLO('yolov9c.pt')\n",
        "\n",
        "# Open the video file\n",
        "video_path = \"DJI_0015.MOV\"\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Store the track history\n",
        "track_history = defaultdict(lambda: [])\n",
        "\n",
        "# Loop through the video frames\n",
        "while cap.isOpened():\n",
        "    # Read a frame from the video\n",
        "    success, frame = cap.read()\n",
        "\n",
        "    if success:\n",
        "        # Run YOLOv8 tracking on the frame, persisting tracks between frames\n",
        "        results = model.track(frame, persist=True, conf=0.2)\n",
        "\n",
        "        # Get the boxes and track IDs\n",
        "        boxes = results[0].boxes.xywh.cpu()\n",
        "        track_ids = results[0].boxes.id.int().cpu().tolist()\n",
        "\n",
        "        # Visualize the results on the frame\n",
        "        annotated_frame = results[0].plot()\n",
        "\n",
        "        # # Plot the tracks\n",
        "        # for box, track_id in zip(boxes, track_ids):\n",
        "        #     x, y, w, h = box\n",
        "        #     track = track_history[track_id]\n",
        "        #     track.append((float(x), float(y)))  # x, y center point\n",
        "        #     if len(track) > 30:  # retain 90 tracks for 90 frames\n",
        "        #         track.pop(0)\n",
        "\n",
        "        #     # Draw the tracking lines\n",
        "        #     points = np.hstack(track).astype(np.int32).reshape((-1, 1, 2))\n",
        "        #     cv2.polylines(annotated_frame, [points], isClosed=False, color=(230, 230, 230), thickness=10)\n",
        "        writer.write(result_img)\n",
        "        # Display the annotated frame\n",
        "        cv2.imshow(\"YOLOv8 Tracking\", cv2.resize(annotated_frame, (1920, 1080)))\n",
        "\n",
        "        # Break the loop if 'q' is pressed\n",
        "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
        "            break\n",
        "    else:\n",
        "        # Break the loop if the end of the video is reached\n",
        "        break\n",
        "\n",
        "# Release the video capture object and close the display window\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
